[
  {
    "title": "[UI-FEATURE] Mobile-First PWA Shell & Navigation",
    "body": "## Epic\nEPIC: IDE & Developer Experience (PrismCode Shell)\n\n## User Story\nAs a developer on the go, I want a mobile-first PrismCode UI that installs like an app and works well on phones and tablets so I can manage projects anywhere.\n\n## Acceptance Criteria\n- [ ] Web app manifest configured (name, icons, start_url, display=standalone)\n- [ ] Service worker serving an app shell for core routes (dashboard, workspace, settings)\n- [ ] Mobile layout with bottom navigation and large touch targets\n- [ ] Desktop layout with sidebar navigation\n- [ ] Basic offline UX for recently visited screens with a custom offline page\n- [ ] Follows core PWA best practices for responsiveness, accessibility, and app-like behavior\n\n## Implementation Notes\n- Add a web app manifest and link it from the HTML head.\n- Implement a service worker that precaches shell assets and intercepts navigation to show a custom offline page when needed.\n- Use responsive layout primitives to seamlessly switch between mobile and desktop navigation patterns.\n- Design URLs and routing to support deep links into key app sections.",
    "labels": ["story", "frontend", "pwa", "ui", "epic:ide-experience", "phase:2"]
  },
  {
    "title": "[UI-FEATURE] PWA Offline UX & Push Notifications",
    "body": "## Epic\nEPIC: IDE & Developer Experience (PrismCode Shell)\n\n## User Story\nAs a busy developer, I want PrismCode to keep working when my connection is flaky and notify me when important events occur so I do not miss builds, reviews, or agent actions.\n\n## Acceptance Criteria\n- [ ] Cache key workflows for offline use (project list, recent issues, last workspace state)\n- [ ] Queue API mutations (comments, task updates) and sync when back online\n- [ ] Visual indicators for offline/online status in the UI\n- [ ] Push notifications for build completion, PR review, agent blocked, and agent completed\n- [ ] User-level notification preferences per event type\n\n## Implementation Notes\n- Extend the existing service worker to cache JSON responses for selected routes and provide a better offline experience.\n- Implement an offline queue using IndexedDB/local storage for pending writes.\n- Use the Notifications API and backend push infrastructure so installed PWAs behave like native apps.",
    "labels": ["story", "frontend", "pwa", "notifications", "epic:ide-experience", "phase:2"]
  },
  {
    "title": "[UI-FEATURE] Visual Workflow Builder Canvas",
    "body": "## Epic\nEPIC: Automation & Workflows (FlowForge/DevFlow Engine)\n\n## User Story\nAs a non-technical PM, I want a visual canvas to design workflows by connecting triggers and actions so I can automate processes without writing code.\n\n## Acceptance Criteria\n- [ ] Canvas view using a graph library (e.g., React Flow) for nodes and edges\n- [ ] Node types: Trigger, Action, Condition\n- [ ] Node palette with common nodes (GitHub event, HTTP call, Slack message, AI step)\n- [ ] Drag-and-drop placement, pan, zoom, and connector drawing between nodes\n- [ ] Snap-to-grid and basic auto-layout where possible\n- [ ] Validation for invalid workflows (unconnected nodes, missing inputs)\n\n## Implementation Notes\n- Represent workflows as JSON (nodes, edges, metadata) compatible with the backend workflow engine.\n- Follow established patterns from real workflow editor templates built on React Flow.\n- Keep node rendering and node schemas extensible for future node types.",
    "labels": ["story", "frontend", "automation", "workflow", "epic:automation-engine", "phase:3"]
  },
  {
    "title": "[UI-FEATURE] Workflow Node Configuration & Run History",
    "body": "## Epic\nEPIC: Automation & Workflows (FlowForge/DevFlow Engine)\n\n## User Story\nAs a workflow builder user, I want to configure individual nodes and inspect run history so I can debug and refine automations.\n\n## Acceptance Criteria\n- [ ] Right-side configuration panel that edits properties of the selected node\n- [ ] Config forms for GitHub, Slack, HTTP, and AI nodes (auth, params, payloads)\n- [ ] Inline validation and helper text for required fields\n- [ ] Run history tab with recent executions and status (success, failed, running)\n- [ ] Ability to open a run and see step-by-step node results and error messages\n- [ ] \"Test node\" action that executes a single node with sample input\n\n## Implementation Notes\n- Drive configuration UI from node schemas defined on the backend so nodes remain consistent across projects.\n- Read run/step data from the workflow execution engineâ€™s history tables.\n- Plan for multi-tenant support in how run history is fetched and displayed.",
    "labels": ["story", "frontend", "automation", "workflow", "epic:automation-engine", "phase:3"]
  },
  {
    "title": "[UI-FEATURE] Embedded Preview Sandbox Panel",
    "body": "## Epic\nEPIC: IDE & Developer Experience (In-Browser IDE)\n\n## User Story\nAs a developer, I want to see a live preview of my app next to my code so I can iterate faster without leaving PrismCode.\n\n## Acceptance Criteria\n- [ ] Split view layout: Monaco editor on the left, preview panel on the right\n- [ ] Live reload in the preview when project files change\n- [ ] Device presets for mobile, tablet, and desktop with adjustable viewport sizes\n- [ ] Console/log panel showing runtime errors and console output from the preview\n- [ ] Error overlay when the preview fails to build or crashes\n\n## Implementation Notes\n- Either embed a browser-based sandbox (WebContainer-style) or proxy to a containerized dev server for previews.\n- Ensure per-user/session isolation for security.\n- Integrate with the deployment/sandbox backend to select the correct environment.",
    "labels": ["story", "frontend", "sandbox", "developer-tools", "epic:ide-experience", "phase:3"]
  },
  {
    "title": "[UI-FEATURE] Sandbox Environment Selector",
    "body": "## Epic\nEPIC: IDE & Developer Experience (In-Browser IDE)\n\n## User Story\nAs a developer, I want to choose which environment the embedded preview uses so I can switch between local dev, shared dev, and staging easily.\n\n## Acceptance Criteria\n- [ ] Environment selector component (Local Dev, Cloud Dev, Staging, Custom)\n- [ ] Status indicators per environment (idle, building, running, failed)\n- [ ] Link from the selector to environment logs/build pages\n- [ ] Graceful handling when an environment is not yet provisioned or unhealthy\n\n## Implementation Notes\n- Fetch environment metadata (URL, status, last deploy) from the deployments backend.\n- Make environment selection persistent at project + user level where appropriate.\n- Treat this as the UI counterpart to deployment/test integration in the roadmap.",
    "labels": ["story", "frontend", "sandbox", "deployment", "epic:ide-experience", "phase:3"]
  },
  {
    "title": "[UI-FEATURE] AI Model Selection & Routing UI",
    "body": "## Epic\nEPIC: AI Agents & Intelligence (Model Orchestration)\n\n## User Story\nAs a power user, I want to choose which AI models are used for different tasks so I can balance cost, speed, and quality.\n\n## Acceptance Criteria\n- [ ] Models table listing provider, model name, context window, and rough cost band\n- [ ] Per-task defaults for code generation, review, docs, tests, and chat\n- [ ] Per-project routing policy toggle (optimize for cost, speed, or quality)\n- [ ] Basic usage dashboard (recent calls, token estimates per model)\n\n## Implementation Notes\n- Drive the UI from the backend model registry exposed by the AI orchestration service.\n- Surfacing simple presets first reduces cognitive load; advanced policies can be added later.\n- Clearly indicate which models are recommended or experimental.",
    "labels": ["story", "frontend", "ai", "multi-model", "epic:ai-orchestration", "phase:2"]
  },
  {
    "title": "[UI-FEATURE] Code Review Assistant Panel",
    "body": "## Epic\nEPIC: AI Agents & Intelligence (Code Review)\n\n## User Story\nAs a reviewer, I want AI-generated review feedback for each PR so I can focus on high-impact changes instead of boilerplate comments.\n\n## Acceptance Criteria\n- [ ] Panel showing PR summary with overall score and risk level\n- [ ] Sections for Critical Issues, Warnings, Positives, and Suggestions\n- [ ] Each finding links to the relevant file and line range in the diff\n- [ ] Filters by severity and category (security, performance, style, tests)\n- [ ] Actions to apply suggestion (if safe), add as PR comment, or mark ignored\n\n## Implementation Notes\n- Consume structured findings from the backend code review engine.\n- Respect user control by keeping automatic commenting opt-in.\n- Log which suggestions are accepted or rejected to refine future recommendations.",
    "labels": ["story", "frontend", "code-review", "ai", "epic:code-review", "phase:2"]
  },
  {
    "title": "[UI-FEATURE] Testing & Quality Dashboard",
    "body": "## Epic\nEPIC: Quality, Testing & Analytics\n\n## User Story\nAs a QA engineer, I want a single place to see test status, coverage, and flakiness so I can quickly spot quality risks.\n\n## Acceptance Criteria\n- [ ] Summary cards per test type (unit, integration, E2E, visual) for the selected project\n- [ ] Timeline of recent test runs with status and duration\n- [ ] Flaky test list with failure rate and last failure time\n- [ ] Coverage trend charts by module or directory\n- [ ] Links into CI logs and artifacts for failed runs\n\n## Implementation Notes\n- Backed by the test execution and results aggregation service.\n- Design visualizations that remain readable as project size grows.\n- Lay the groundwork for cross-project quality views later.",
    "labels": ["story", "frontend", "testing", "analytics", "epic:quality-testing", "phase:2"]
  },
  {
    "title": "[UI-FEATURE] Data Visualization Studio",
    "body": "## Epic\nEPIC: Quality, Testing & Analytics\n\n## User Story\nAs a team lead, I want to build custom dashboards combining issues, tests, deployments, and agent metrics so I can track what matters to my team.\n\n## Acceptance Criteria\n- [ ] Dashboard builder with drag-and-drop widgets (charts, tables, KPI cards)\n- [ ] Support for line, bar, area, and pie charts plus table views\n- [ ] Widget configuration UI that binds to predefined data sources (issues, tests, deploys, agents)\n- [ ] Ability to save dashboards and share them at project and organization level\n- [ ] Responsive grid layout for arranging and resizing widgets\n\n## Implementation Notes\n- Use a proven charting library with strong time-series support.\n- Keep data queries behind a small set of analytics endpoints to avoid coupling dashboards to raw schemas.\n- Provide a few starter templates (Delivery, Quality, Agent Performance).",
    "labels": ["story", "frontend", "visualization", "analytics", "epic:quality-testing", "phase:3"]
  },
  {
    "title": "[UI-FEATURE] Integrations Hub Catalog",
    "body": "## Epic\nEPIC: Integrations & Platform\n\n## User Story\nAs a user, I want a catalog of integrations (GitHub, GitLab, Jira, Slack, Vercel, etc.) so I can connect the tools my team already uses.\n\n## Acceptance Criteria\n- [ ] Grid/list of integrations with logos, categories, and short descriptions\n- [ ] Detail page per integration with scopes, capabilities, and example workflows\n- [ ] Connect/disconnect button with clear auth state (connected, needs setup, error)\n- [ ] Last health check status and timestamp for each active connection\n\n## Implementation Notes\n- Drive the catalog from the backend integration registry.\n- Present security-sensitive information (scopes, permissions) clearly during connect.\n- Link from integration detail pages into relevant workflow templates.",
    "labels": [
      "story",
      "frontend",
      "integrations",
      "platform",
      "epic:integrations-platform",
      "phase:2"
    ]
  },
  {
    "title": "[BACKEND] Multi-Model AI Orchestration Service",
    "body": "## Epic\nEPIC: AI Agents & Intelligence (Model Orchestration)\n\n## Technical Story\nBuild a backend service that routes all AI requests to the best model based on task type, cost, and performance.\n\n## Requirements\n- [ ] Model registry containing provider, model name, context size, latency expectations, and pricing band\n- [ ] Routing policies per task (code generation, review, docs, tests, chat) with pluggable strategies (cost, speed, quality)\n- [ ] Fallback logic with retries when primary models fail or hit rate limits\n- [ ] Per-request logging of model, tokens, latency, and outcome for analytics\n- [ ] Internal APIs for other services and the UI (e.g., /internal/ai/complete, /internal/ai/review)\n\n## Notes\n- Start with a small set of stable models and providers.\n- Keep adapter interfaces clean so new providers can be added without large refactors.",
    "labels": ["story", "backend", "ai", "service", "epic:ai-orchestration", "phase:1"]
  },
  {
    "title": "[BACKEND] Code Review Analysis Engine",
    "body": "## Epic\nEPIC: AI Agents & Intelligence (Code Review)\n\n## Technical Story\nImplement a backend engine that ingests pull request diffs and produces structured findings for the Code Review Assistant UI and optional PR comments.\n\n## Requirements\n- [ ] Endpoint to accept diffs and PR metadata (repo, branch, PR number, author)\n- [ ] Static checks for security, performance, style, and testing coverage gaps\n- [ ] AI-based analysis for higher-level feedback (design issues, missing tests, risky changes)\n- [ ] Structured JSON output (overall score, severity, category, file/line ranges, suggestions)\n- [ ] Optional integration that posts summarized comments directly on the PR via the Git provider API\n\n## Notes\n- Focus on a predictable, machine-readable schema; natural language can be rendered in the UI.\n- Respect rate limits and organization policies when writing back to Git providers.",
    "labels": ["story", "backend", "code-review", "ai", "epic:code-review", "phase:1"]
  },
  {
    "title": "[BACKEND] Test Execution & Results Aggregation Service",
    "body": "## Epic\nEPIC: Quality, Testing & Analytics\n\n## Technical Story\nProvide a central backend service to ingest test results from CI pipelines and expose them to the Testing Dashboard and analytics.\n\n## Requirements\n- [ ] API to record test runs with project, commit, branch, suite type, status, duration, and environment\n- [ ] Storage of individual test case results with failure messages and optional artifacts (log URLs, screenshots)\n- [ ] Detection of flaky tests based on historical pass/fail patterns\n- [ ] Summary and detail endpoints for use by dashboards and drill-down views\n\n## Notes\n- Support multiple CI providers by keeping the ingestion format simple and documented.\n- Plan retention and archival strategies for long-lived projects.",
    "labels": ["story", "backend", "testing", "service", "epic:quality-testing", "phase:1"]
  },
  {
    "title": "[BACKEND] Integration Connector Framework",
    "body": "## Epic\nEPIC: Integrations & Platform\n\n## Technical Story\nCreate a backend framework for defining and managing third-party integrations and their credentials, used by workflows and the Integrations Hub.\n\n## Requirements\n- [ ] Integration registry describing id, name, provider, auth type, scopes, and capabilities (triggers/actions)\n- [ ] OAuth2 and API key-based authentication flows with secure token storage\n- [ ] Health check mechanism for each integration (e.g., lightweight API call)\n- [ ] Common calling convention for connector actions/triggers used by the workflow engine\n\n## Notes\n- Use strong encryption and least-privilege access for stored credentials.\n- Model organizations/tenants explicitly to support multiple customers safely.",
    "labels": [
      "story",
      "backend",
      "integrations",
      "service",
      "epic:integrations-platform",
      "phase:1"
    ]
  },
  {
    "title": "[BACKEND] Real-Time Collaboration Service",
    "body": "## Epic\nEPIC: IDE & Developer Experience (Collaboration)\n\n## Technical Story\nBuild a backend service that powers real-time collaboration, presence, and shared workspaces across the IDE and workflow views.\n\n## Requirements\n- [ ] WebSocket gateway for connection lifecycle, heartbeats, and authentication\n- [ ] Room model for workspaces, files, and sessions (per-project and per-document rooms)\n- [ ] Presence tracking (who is online and which resource they are viewing)\n- [ ] Streaming of live cursor updates and CRDT-based document synchronization\n- [ ] Horizontal scaling (e.g., Redis pub/sub) for multiple instances\n\n## Notes\n- Start with presence and cursor sharing, then layer in full collaborative editing using mature CRDT approaches.\n- Follow proven patterns for real-time collaborative editors to keep latency low and conflict resolution robust.",
    "labels": ["story", "backend", "realtime", "collaboration", "epic:ide-experience", "phase:1"]
  }
]
